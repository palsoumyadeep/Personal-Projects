# -*- coding: utf-8 -*-
"""TVS Case 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z0b4FwAVxnh_LQTw1EYh8kj1mVebiR9X
"""

import numpy as np

import pandas as pd
df = pd.read_csv("/content/TVS_Case (1).csv")
df.dropna(inplace = True)

df.head()

df.drop("GMM_AUTO", axis = 1 ,inplace = True)



from datetime import datetime
df["Empanel_Date"] = pd.to_datetime(df["Empanel_Date"])
df["Latest_Dealer_Date"] = pd.to_datetime(df["Latest_Dealer_Date"])

df['MonthsTillEmpanelled'] = (datetime.now() - df["Empanel_Date"]).astype('timedelta64[M]')
df["MonthsTill_Last_Deal"] = (datetime.now() - df["Latest_Dealer_Date"]).astype('timedelta64[M]')

df_encoded = df.copy()
categorical_columns = ["State","Region","Manf_ID","City_Tier","Dealer_Type","Affluence_Category","Product_Sold"]

from sklearn.preprocessing import LabelEncoder
# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Iterate over each categorical column and encode it
for col in categorical_columns:
    df_encoded[col] = label_encoder.fit_transform(df_encoded[col])

df_encoded.drop([df_encoded.columns[0]],axis = 1, inplace = True)

df_encoded.head()

df_encoded.drop(["UID","Empanel_Date","Latest_Dealer_Date"], axis = 1, inplace = True)

na_counts = df_encoded.isna().sum()
na_counts

df_active = df_encoded[df_encoded["KMEANS_AUTO"] == 1]
df_passive = df_encoded[df_encoded["KMEANS_AUTO"] == 0]

df_sales = df_active.sort_values(by='Total_Sales_2Y', ascending=False)[:3][["State","Region","Manf_ID","City_Tier","Dealer_Type","Affluence_Category","Product_Sold","MonthsTillEmpanelled","MonthsTill_Last_Deal"]]
df_customers =  df_active.sort_values(by='Total_Customers_2Y', ascending=False)[:3][["State","Region","Manf_ID","City_Tier","Dealer_Type","Affluence_Category","Product_Sold","MonthsTillEmpanelled","MonthsTill_Last_Deal"]]
df_cross_sell = df_active.sort_values(by='Cross_Sell_Customers_2Y', ascending=False)[:3][["State","Region","Manf_ID","City_Tier","Dealer_Type","Affluence_Category","Product_Sold","MonthsTillEmpanelled","MonthsTill_Last_Deal"]]

df_active_1 = df_active[["State","Region","Manf_ID","City_Tier","Dealer_Type","Affluence_Category","Product_Sold","MonthsTillEmpanelled","MonthsTill_Last_Deal"]]

df_active_1.head()

from sklearn.metrics.pairwise import cosine_similarity

def calculate_average_cosine_similarity(main_df, target_df,col_name):
    # Initialize an empty list to store the average similarity values
    avg_cosine_similarities = []

    # Iterate through each row in the main dataframe
    for _, main_row in main_df.iterrows():
        # Flatten the values of the main row and reshape it to a 2D array
        main_vector = main_row.values.reshape(1, -1)

        # Calculate cosine similarity with each row in the target dataframe
        similarities = cosine_similarity(main_vector, target_df.values)

        # Calculate the average similarity and append it to the list
        avg_similarity = np.mean(similarities)
        avg_cosine_similarities.append(avg_similarity)

    # Create a new column in the main dataframe to store the average similarities
    main_df[col_name] = avg_cosine_similarities
    return main_df

df_active_sales= calculate_average_cosine_similarity(df_active_1,df_sales,"SALES")

df_active_Customer = calculate_average_cosine_similarity(df_active_1,df_customers,"CUSTOMER")
df_active_CROSS = calculate_average_cosine_similarity(df_active_1,df_cross_sell,"CROSS_SALES")



df_active_sales.head()

df_active_sales = df_active_sales[df_active_sales["SALES"] >= 0.99]

df_active_sales.shape

df.shape

df_active_sales.shape

df_active.shape

df_un_active = df[df["KMEANS_AUTO"] == 1]
df_un_active["SALES"] = df_active_sales["SALES"]
df_un_active["CUSTOMER"] = df_active_Customer["CUSTOMER"]
df_un_active["CROSS_SALES"] = df_active_CROSS["CROSS_SALES"]

df_un_active.head()

df_un_active_sales = df_un_active[df_un_active["SALES"] >= 0.99]
df_un_active_customer = df_un_active[df_un_active["CUSTOMER"] >= 0.99]
df_un_active_CROSS = df_un_active[df_un_active["CROSS_SALES"] >= 0.99]

df_un_active_sales.columns

import matplotlib.pyplot as plt

# Assuming `df` is your dataframe

# Step 1: Filter rows where SALES >= 0.99
df_filtered = df_un_active[df_un_active['SALES'] >= 0.99]

# Step 4: Identify the highest occurring state
highest_occurring_state = df_filtered['State'].value_counts().idxmax()

# Step 5: Filter rows for the highest occurring state and identify the top 2 occurring regions
top_regions = df_filtered[df_filtered['State'] == highest_occurring_state]['Region'].value_counts().nlargest(2).index.tolist()

# Step 6: Filter rows for the top regions and identify the highest occurring city tier
highest_occurring_city_tier = df_filtered[df_filtered['Region'].isin(top_regions)]['City_Tier'].value_counts().idxmax()

# Step 7: Filter rows for the highest occurring city tier and identify the highest occurring Affluence Category, Manf_ID, and Product_Sold
final_result = df_filtered[df_filtered['City_Tier'] == highest_occurring_city_tier]
final_result = final_result[['Affluence_Category', 'Manf_ID', 'Product_Sold']].value_counts().idxmax()

# Create a bar plot
labels = ['Highest Occurring State', 'Top 2 Regions in Highest State', 'Highest Occurring City Tier',
          'Highest Occurring Affluence Category', 'Highest Occurring Manf_ID', 'Highest Occurring Product_Sold']

values = [highest_occurring_state, top_regions, highest_occurring_city_tier,
          final_result[0], final_result[1], final_result[2]]

plt.figure(figsize=(10, 6))
plt.barh(labels, [len(item) if isinstance(item, list) else 1 for item in values], color='skyblue')
plt.xlabel('Count')
plt.title('Steps to Identify Highest Occurring Combinations')
plt.show()

# Assuming `df` is your dataframe

# Step 1: Filter rows where SALES >= 0.99
df_filtered = df_un_active[df_un_active['SALES'] >= 0.99]

# Step 2-3: Group and count occurrences
grouped = df_filtered.groupby(['State', 'Region', 'City_Tier', 'Affluence_Category', 'Manf_ID', 'Product_Sold']).size().reset_index(name='count')

# Step 4: Identify the highest occurring state
highest_occuring_state = grouped['State'].value_counts().idxmax()

# Step 5: Filter rows for the highest occurring state and identify the top 2 occurring regions
top_regions = grouped[grouped['State'] == highest_occuring_state]['Region'].value_counts().nlargest(2).index.tolist()

# Step 6: Filter rows for the top regions and identify the highest occurring city tier
highest_occuring_city_tier = grouped[grouped['Region'].isin(top_regions)]['City_Tier'].value_counts().idxmax()

# Step 7: Filter rows for the highest occurring city tier and identify the highest occurring Affluence Category, Manf_ID, and Product_Sold
final_result = grouped[grouped['City_Tier'] == highest_occuring_city_tier]
final_result = final_result[['Affluence_Category', 'Manf_ID', 'Product_Sold']].value_counts().idxmax()

grouped.sort_values(by = 'count',ascending = False)

# Assuming `df` is your dataframe

# Step 1: Filter rows where SALES >= 0.99
df_filtered = df_un_active[df_un_active['CROSS_SALES'] >= 0.99]

# Step 2-3: Group and count occurrences
grouped = df_filtered.groupby(['State', 'Region', 'City_Tier', 'Affluence_Category', 'Manf_ID', 'Product_Sold']).size().reset_index(name='count')

# Step 4: Identify the highest occurring state
highest_occuring_state = grouped['State'].value_counts().idxmax()

# Step 5: Filter rows for the highest occurring state and identify the top 2 occurring regions
top_regions = grouped[grouped['State'] == highest_occuring_state]['Region'].value_counts().nlargest(2).index.tolist()

# Step 6: Filter rows for the top regions and identify the highest occurring city tier
highest_occuring_city_tier = grouped[grouped['Region'].isin(top_regions)]['City_Tier'].value_counts().idxmax()

# Step 7: Filter rows for the highest occurring city tier and identify the highest occurring Affluence Category, Manf_ID, and Product_Sold
final_result = grouped[grouped['City_Tier'] == highest_occuring_city_tier]
final_result = final_result[['Affluence_Category', 'Manf_ID', 'Product_Sold']].value_counts().idxmax()

grouped.sort_values(by = 'count',ascending = False)

# Assuming `df` is your dataframe

# Step 1: Filter rows where SALES >= 0.99
df_filtered = df_un_active[df_un_active['CUSTOMER'] >= 0.99]

# Step 2-3: Group and count occurrences
grouped = df_filtered.groupby(['State', 'Region', 'City_Tier', 'Affluence_Category', 'Manf_ID', 'Product_Sold']).size().reset_index(name='count')

# Step 4: Identify the highest occurring state
highest_occuring_state = grouped['State'].value_counts().idxmax()

# Step 5: Filter rows for the highest occurring state and identify the top 2 occurring regions
top_regions = grouped[grouped['State'] == highest_occuring_state]['Region'].value_counts().nlargest(2).index.tolist()

# Step 6: Filter rows for the top regions and identify the highest occurring city tier
highest_occuring_city_tier = grouped[grouped['Region'].isin(top_regions)]['City_Tier'].value_counts().idxmax()

# Step 7: Filter rows for the highest occurring city tier and identify the highest occurring Affluence Category, Manf_ID, and Product_Sold
final_result = grouped[grouped['City_Tier'] == highest_occuring_city_tier]
final_result = final_result[['Affluence_Category', 'Manf_ID', 'Product_Sold']].value_counts().idxmax()

grouped.sort_values(by = 'count',ascending = False)

df_un_passive = df[df["KMEANS_AUTO"] == 0]

df_un_passive.head()

df_passive_1 = df_passive[["State","Region","Manf_ID","City_Tier","Dealer_Type","Affluence_Category","Product_Sold","MonthsTillEmpanelled","MonthsTill_Last_Deal"]]
df_passive_1

df_passive_sales = calculate_average_cosine_similarity(df_passive_1,df_sales,"SALES")

df_passive_customer = calculate_average_cosine_similarity(df_passive_1,df_customers,"CUSTOMER")

df_passive_cross_sale = calculate_average_cosine_similarity(df_passive_1,df_cross_sell,"CROSS_SALES")

df_un_passive["SALES"] = df_passive_sales["SALES"]
df_un_passive["CUSTOMER"] = df_passive_customer["CUSTOMER"]
df_un_passive["CROSS_SALES"] = df_passive_cross_sale["CROSS_SALES"]

# Assuming `df` is your dataframe

# Step 1: Filter rows where SALES >= 0.99
df_filtered = df_un_passive[df_un_passive['SALES'] >= 0.988]

# Step 2-3: Group and count occurrences
grouped = df_filtered.groupby(['State', 'Region', 'City_Tier', 'Affluence_Category', 'Manf_ID', 'Product_Sold']).size().reset_index(name='count')

# Step 4: Identify the highest occurring state
highest_occuring_state = grouped['State'].value_counts().idxmax()

# Step 5: Filter rows for the highest occurring state and identify the top 2 occurring regions
top_regions = grouped[grouped['State'] == highest_occuring_state]['Region'].value_counts().nlargest(2).index.tolist()

# Step 6: Filter rows for the top regions and identify the highest occurring city tier
highest_occuring_city_tier = grouped[grouped['Region'].isin(top_regions)]['City_Tier'].value_counts().idxmax()

# Step 7: Filter rows for the highest occurring city tier and identify the highest occurring Affluence Category, Manf_ID, and Product_Sold
final_result = grouped[grouped['City_Tier'] == highest_occuring_city_tier]
final_result = final_result[['Affluence_Category', 'Manf_ID', 'Product_Sold']].value_counts().idxmax()

grouped.sort_values(by = 'count',ascending = False)[:3]

# Assuming `df` is your dataframe

# Step 1: Filter rows where SALES >= 0.99
df_filtered = df_un_passive[df_un_passive['CUSTOMER'] >= 0.97]

# Step 2-3: Group and count occurrences
grouped = df_filtered.groupby(['State', 'Region', 'City_Tier', 'Affluence_Category', 'Manf_ID', 'Product_Sold']).size().reset_index(name='count')

# Step 4: Identify the highest occurring state
highest_occuring_state = grouped['State'].value_counts().idxmax()

# Step 5: Filter rows for the highest occurring state and identify the top 2 occurring regions
top_regions = grouped[grouped['State'] == highest_occuring_state]['Region'].value_counts().nlargest(2).index.tolist()

# Step 6: Filter rows for the top regions and identify the highest occurring city tier
highest_occuring_city_tier = grouped[grouped['Region'].isin(top_regions)]['City_Tier'].value_counts().idxmax()

# Step 7: Filter rows for the highest occurring city tier and identify the highest occurring Affluence Category, Manf_ID, and Product_Sold
final_result = grouped[grouped['City_Tier'] == highest_occuring_city_tier]
final_result = final_result[['Affluence_Category', 'Manf_ID', 'Product_Sold']].value_counts().idxmax()

grouped.sort_values(by = 'count',ascending = False)[:3]

# Assuming `df` is your dataframe

# Step 1: Filter rows where SALES >= 0.99
df_filtered = df_un_passive[df_un_passive['CROSS_SALES'] >= 0.98]

# Step 2-3: Group and count occurrences
grouped = df_filtered.groupby(['State', 'Region', 'City_Tier', 'Affluence_Category', 'Manf_ID', 'Product_Sold']).size().reset_index(name='count')

# Step 4: Identify the highest occurring state
highest_occuring_state = grouped['State'].value_counts().idxmax()

# Step 5: Filter rows for the highest occurring state and identify the top 2 occurring regions
top_regions = grouped[grouped['State'] == highest_occuring_state]['Region'].value_counts().nlargest(2).index.tolist()

# Step 6: Filter rows for the top regions and identify the highest occurring city tier
highest_occuring_city_tier = grouped[grouped['Region'].isin(top_regions)]['City_Tier'].value_counts().idxmax()

# Step 7: Filter rows for the highest occurring city tier and identify the highest occurring Affluence Category, Manf_ID, and Product_Sold
final_result = grouped[grouped['City_Tier'] == highest_occuring_city_tier]
final_result = final_result[['Affluence_Category', 'Manf_ID', 'Product_Sold']].value_counts().idxmax()

grouped.sort_values(by = 'count',ascending = False)[:3]

