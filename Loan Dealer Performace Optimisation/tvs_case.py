# -*- coding: utf-8 -*-
"""TVS Case.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w6BtlOx_bVLhqXhg9WmGesEkdZ7g3h2F
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_excel("/content/drive/MyDrive/TVS Case/EPIC_Analytics Case2_Dealer Segmentation.xlsx")

from scipy import stats

df.head()

# Shorter column names
short_column_names = ['UID',
                      'Empanel_Date',
                      'State',
                      'Region',
                      'Manf_ID',
                      'City_Tier',
                      'Latest_Dealer_Date',
                      'Total_Customers_2Y',
                      'Total_Sales_2Y',
                      'Sales_Months_2Y',
                      'Affluence_Category',
                      'Cross_Sell_Customers_2Y',
                      'ABND_Cases_6M',
                      'Disbursed_Cases_6M',
                      'Normal_Scheme_Cases_6M',
                      'ETC_Cases_6M',
                      'Dealer_Logins_6M',
                      'EMIS_Collected_3M',
                      'EMIS_Moved_Higher_Bucket_3M',
                      'Outstanding_EMIS_3M',
                      'Outstanding_30_DPD_EMIS_3M',
                      'Outstanding_90_DPD_EMIS_3M',
                      'Bounced_EMIS_3M',
                      'Fraud_Score',
                      'Dealer_Type',
                      'Product_Sold']


# Rename the columns with shorter names
df.columns = short_column_names

df.head()

na_counts = df.isna().sum()
na_counts

df.dropna(subset=['Manf_ID', 'City_Tier'], inplace=True)

df_1 = df.drop(["UID","Fraud_Score","Dealer_Type","Product_Sold"],axis = 1)

df_1.head()

from datetime import datetime
df_1["Empanel_Date"] = pd.to_datetime(df_1["Empanel_Date"])
df_1['MonthsTillEmpanelled'] = (datetime.now() - df_1["Empanel_Date"]).astype('timedelta64[M]')
df_1["MonthsTill_Last_Deal"] = (datetime.now() - df_1["Latest_Dealer_Date"]).astype('timedelta64[M]')

df_1.drop(["Empanel_Date","Latest_Dealer_Date"],axis = 1,inplace =True)

# Create dummy variables for categorical columns and concatenate them to the original DataFrame
df_1 = pd.concat([df_1, pd.get_dummies(df_1[['State', 'Region',"City_Tier",'Affluence_Category']], prefix=['State', 'Region',"City_Tier",'Affluence_Category'])], axis=1)
# Drop the original categorical columns
df_1.drop(['State', 'Region', 'Affluence_Category','Manf_ID', 'City_Tier'], axis=1, inplace=True)

df_1.head()

df_cluster = df_1.copy()

df_cluster.head()

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs

# Generate a sample dataset (replace this with your own dataset)
n_samples = 300
n_features = 2
data, true_labels = make_blobs(n_samples=n_samples, n_features=n_features, centers=2, random_state=42)


# Standardize the data (important for K-Means)
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_cluster)

# Apply K-Means clustering with 2 clusters
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans_labels = kmeans.fit_predict(scaled_data)

# Apply Gaussian Mixture Model (GMM) with 2 clusters
gmm = GaussianMixture(n_components=2)
gmm_labels = gmm.fit_predict(scaled_data)

# Apply PCA for dimensionality reduction to 2 components
pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_data)

# Plot the results for K-Means
plt.figure(figsize=(12, 4))
plt.subplot(121)
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=kmeans_labels, cmap='viridis')
plt.title('K-Means Clustering (2 clusters)')

# Plot the results for GMM
plt.subplot(122)
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=gmm_labels, cmap='viridis')
plt.title('GMM Clustering (2 clusters)')

plt.tight_layout()
plt.show()

import seaborn as sns

correlations = df_1.corr()

f, ax = plt.subplots(figsize = (20, 20))
sns.heatmap(correlations, annot = True)
plt.show()

input_df = df_cluster.copy()

input_df.shape

from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.initializers import glorot_uniform
from keras.optimizers import SGD

encoding_dim = 7

input_df = Input(shape=(60,))


# Glorot normal initializer (Xavier normal initializer) draws samples from a truncated normal distribution

x = Dense(encoding_dim, activation='relu')(input_df)
x = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)
x = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)
x = Dense(2000, activation='relu', kernel_initializer = 'glorot_uniform')(x)

encoded = Dense(70, activation='relu', kernel_initializer = 'glorot_uniform')(x)

x = Dense(2000, activation='relu', kernel_initializer = 'glorot_uniform')(encoded)
x = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)

decoded = Dense(60, kernel_initializer = 'glorot_uniform')(x)

# autoencoder
autoencoder = Model(input_df, decoded)

#encoder - used for our dimention reduction
encoder = Model(input_df, encoded)

autoencoder.compile(optimizer= 'adam', loss='mean_squared_error')

autoencoder.fit(scaled_data, scaled_data, batch_size = 128, epochs = 25,  verbose = 1)

scaled_data.shape

pred_data = encoder.predict(scaled_data)

pred_data.shape

pred_df = pd.DataFrame(pred_data)

pred_df.head()

# Generate a sample dataset (replace this with your own dataset)
n_samples = 300
n_features = 2
data, true_labels = make_blobs(n_samples=n_samples, n_features=n_features, centers=2, random_state=42)


# Standardize the data (important for K-Means)
scaler2 = StandardScaler()
scaled_data_2 = scaler2.fit_transform(pred_df)

# Apply K-Means clustering with 2 clusters
kmeans1 = KMeans(n_clusters=2, random_state=42)
kmeans_labels_1 = kmeans1.fit_predict(scaled_data_1)

# Apply Gaussian Mixture Model (GMM) with 2 clusters
gmm1 = GaussianMixture(n_components=2)
gmm_labels_1 = gmm1.fit_predict(scaled_data_1)

# Apply PCA for dimensionality reduction to 2 components
pca1 = PCA(n_components=2)
pca_data_1 = pca1.fit_transform(scaled_data_1)

# Plot the results for K-Means
plt.figure(figsize=(12, 4))
plt.subplot(121)
plt.scatter(pca_data_1[:, 0], pca_data_1[:, 1], c=kmeans_labels_1, cmap='viridis')
plt.title('K-Means Clustering (2 clusters)')

# Plot the results for GMM
plt.subplot(122)
plt.scatter(pca_data_1[:, 0], pca_data_1[:, 1], c=gmm_labels_1, cmap='viridis')
plt.title('GMM Clustering (2 clusters)')

plt.tight_layout()
plt.show()

# df["KMEANS"] = kmeans_labels
# df["GMM"] = gmm_labels
df["KMEANS_AUTO"] = kmeans_labels_1
df["GMM_AUTO"] = gmm_labels_1

df.head()

df.to_csv("TVS_Case.csv")

target_columns = ['Latest_Dealer_Date', 'Total_Customers_2Y', 'Total_Sales_2Y','Sales_Months_2Y',
                  'Cross_Sell_Customers_2Y','ABND_Cases_6M', 'Disbursed_Cases_6M',
                  'Normal_Scheme_Cases_6M','ETC_Cases_6M', 'Dealer_Logins_6M', 'EMIS_Collected_3M',
       'EMIS_Moved_Higher_Bucket_3M', 'Outstanding_EMIS_3M',
       'Outstanding_30_DPD_EMIS_3M', 'Outstanding_90_DPD_EMIS_3M',
       'Bounced_EMIS_3M', 'Fraud_Score', 'KMEANS_AUTO', 'GMM_AUTO']

df_hyp = df[target_columns]

df_hyp.head()

df_hyp["MonthsTill_Last_Deal"] = (datetime.now() - df_hyp["Latest_Dealer_Date"]).astype('timedelta64[M]')

df_hyp.drop(["Latest_Dealer_Date"],axis = 1,inplace = True)

target_columns = ['Total_Customers_2Y', 'Total_Sales_2Y','Sales_Months_2Y',
                  'Cross_Sell_Customers_2Y','ABND_Cases_6M', 'Disbursed_Cases_6M',
                  'Normal_Scheme_Cases_6M','ETC_Cases_6M', 'Dealer_Logins_6M', 'EMIS_Collected_3M',
       'EMIS_Moved_Higher_Bucket_3M', 'Outstanding_EMIS_3M',
       'Outstanding_30_DPD_EMIS_3M', 'Outstanding_90_DPD_EMIS_3M',
       'Bounced_EMIS_3M', 'Fraud_Score',"MonthsTill_Last_Deal"]

categorical_cols = ['KMEANS_AUTO','GMM_AUTO']

dfs = []
for cat in categorical_cols:
  mean_0 = []
  mean_1 = []
  t_stat = []
  p_val = []
  comparision = []

  for tc in target_columns:
    filt_df = df_hyp[[tc,cat]]
    filt_df.dropna(inplace = True)

    grp_0 = filt_df[df_hyp[cat] == 0][tc]
    grp_1 = filt_df[df_hyp[cat] == 1][tc]

    mean_0.append(grp_0.mean())
    mean_1.append(grp_1.mean())

    if grp_0.mean() > grp_1.mean():
      t_statistic, p_value = stats.ttest_ind(grp_0, grp_1, alternative='greater')
      comparision.append("greater")
    else:
      t_statistic, p_value = stats.ttest_ind(grp_0, grp_1, alternative='less')
      comparision.append("lesser")

    t_stat.append(t_statistic)
    p_val.append(p_value)

  df_hyp_test = pd.DataFrame({"ALGO":[cat]*len(target_columns),"Variable":target_columns,"mean_0" : mean_0, "mean_1":mean_1,"comparision" : comparision ,"p_value" : p_val})
  df_hyp_test["is_significant"] = df_hyp_test['p_value'].apply(lambda x: 1 if x < 0.05 else 0)
  dfs.append(df_hyp_test)

df_logistic = df.copy()

df_logistic.head()

df_logistic["Empanel_Date"] = pd.to_datetime(df_logistic["Empanel_Date"])
df_logistic['MonthsTillEmpanelled'] = (datetime.now() - df_logistic["Empanel_Date"]).astype('timedelta64[M]')
df_logistic["MonthsTill_Last_Deal"] = (datetime.now() - df_logistic["Latest_Dealer_Date"]).astype('timedelta64[M]')

df_logistic.head()

# Create dummy variables for categorical columns and concatenate them to the original DataFrame
df_logistic = pd.concat([df_logistic, pd.get_dummies(df_logistic[['State', 'Region',"Manf_ID","City_Tier",'Affluence_Category']], prefix=['State', 'Region',"Manf_ID","City_Tier",'Affluence_Category'])], axis=1)
# Drop the original categorical columns
df_logistic.drop(['State', 'Region', 'Affluence_Category','Manf_ID', 'City_Tier',"UID","Empanel_Date","Latest_Dealer_Date",'Dealer_Type', 'Product_Sold'], axis=1, inplace=True)

df_logistic.columns

import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor

def calculate_vif(data_frame):
    # Get all independent variables
    X = data_frame.drop(columns=['KMEANS_AUTO', 'GMM_AUTO'])
    data_frame.dropna(inplace =  True)
    # Calculate VIF for each variable
    vif_data = pd.DataFrame()
    vif_data["Variable"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    # print(vif_data)
    return vif_data

def drop_highest_vif_column(data_frame):
    vif_data = calculate_vif(data_frame)
    highest_vif_column = vif_data.loc[vif_data["VIF"].idxmax()]["Variable"]
    data_frame = data_frame.drop(columns=highest_vif_column)
    print(f"Dropped column with highest VIF: {highest_vif_column}")
    return data_frame

while True:
    vif_data = calculate_vif(df_logistic.dropna())
    max_vif = vif_data["VIF"].max()
    if max_vif > 5:
        df_logistic = drop_highest_vif_column(df_logistic)
    else:
        print("All VIF scores are below 5.0")
        break

df_logistic.columns



df_kmeans = df_logistic.drop(["GMM_AUTO"], axis = 1).copy()
df_GMM = df_logistic.drop(["KMEANS_AUTO"], axis = 1).copy()

import statsmodels.api as sm

def logistic_regression_summary(df, X_columns, target_column):

    df.dropna(inplace =True)
    # Step 1: Prepare the data
    X = df[X_columns].astype(float)  # Convert to appropriate data type (e.g., float)
    y = df[target_column]
    X = sm.add_constant(X)  # Add a constant term for the intercept

    # Step 2: Fit the logistic regression model
    model = sm.Logit(y, X)
    result = model.fit()

    # Step 3: Display the summary
    print(result.summary())

    # Step 4: Display names of significant variables
    significant_variables = result.pvalues[result.pvalues <= 0.05].index
    print("\nSignificant variables:")
    print(list(significant_variables))

    # Step 5: Check if all variables are significant
    if set(X_columns) == set(significant_variables):
        print("\nAll variables are significant.")
    else:
        print("\nNot all variables are significant.")

logistic_regression_summary(df_kmeans,df_kmeans.columns,["KMEANS_AUTO"])

def display_object_columns(df):
    object_columns = df.select_dtypes(include=['object']).columns
    return object_columns

display_object_columns(df_logistic)

calculate_vif(df_logistic)

